{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgpgit/WebScraping/blob/main/Englishnow_podcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "d8lt28ecU0QC",
        "outputId": "ea12786d-cb6c-40ce-b6ed-e4b5290b6518"
      },
      "outputs": [],
      "source": [
        "# Utilizando os conceitos abordados por esse mago da programação, foi desenvolvido um script para a varredura dos arquivos do melhor podcast de inglês que eu conheço...\n",
        "# divirtam-se. \n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('qx2LGtKzjxk', width= 640,height=360)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KHMLGSG4VS46"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eQvZU9nyZXON"
      },
      "outputs": [],
      "source": [
        "# Constantes: Urlbase && Cabeçalho\n",
        "URL_BASE = 'https://speakenglishpodcast.com/podcast/'\n",
        "HEADERS = {\n",
        "        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9DJG1JuuNcWo"
      },
      "outputs": [],
      "source": [
        "# Carregar todo o html da pagina\n",
        "def buscar_conteudo (url, cabecalho):\n",
        "  site = requests.get(url, headers=cabecalho)\n",
        "  soup = BeautifulSoup(site.content, 'html.parser')\n",
        "  return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dbMaFk3VIckm"
      },
      "outputs": [],
      "source": [
        "# Salvar os arquivos em disco local\n",
        "def salvar_arquivo(link, file_name):\n",
        "  try: \n",
        "    r = requests.get(link)\n",
        "    with open(file_name, 'wb') as f:  \n",
        "      f.write(r.content)  \n",
        "      print( \"%s downloaded!\\n\"%file_name )\n",
        "  except:\n",
        "    print ('erro: ', link )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URaP_C1aOCxt",
        "outputId": "ee7c891b-6d81-4bff-915b-d84cda40c0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41\n"
          ]
        }
      ],
      "source": [
        "# Verificar quantas paginas existem para navegar\n",
        "last_page_link = buscar_conteudo(URL_BASE, HEADERS)\n",
        "last_page_link = last_page_link.find('a', class_='last')['href'] # filtra o conteúdo das url por classe\n",
        "ultima_pagina = last_page_link.split('/')[-2]\n",
        "\n",
        "print(ultima_pagina)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvTs-8vfbj64"
      },
      "outputs": [],
      "source": [
        "# Scraping\n",
        "for i in reversed(range(1,int(ultima_pagina) + 1)): #varredura da pagina iniciando da ultima para a primeira \"reversed\"\n",
        "  url_pag = URL_BASE + f'page/{i}' #url da página\n",
        "  print(\"url_pag: \", url_pag)\n",
        "  url_artigos = buscar_conteudo(url_pag, HEADERS)#obter a página dos artigos\n",
        "  url_artigos = url_artigos.find_all('h2', class_='entry-title') # filtra o conteúdo das url's dos artigos por classe\n",
        "  for cell in url_artigos: # separar os links dos artigos\n",
        "    link = cell.find('a')['href']\n",
        "    url_arquivo = link\n",
        "    links = buscar_conteudo(url_arquivo, HEADERS)#obter a página dos arquivos\n",
        "    links = links.find_all('li')\n",
        "    qtd_links = len(links) # contabilizar a quantidade de arquivos para download\n",
        "    for i in range(0,qtd_links): # baixar apenas arquivos mp3 e pdf\n",
        "      texto = str(links[i])\n",
        "      if 'href' and ('.mp3') in texto:\n",
        "        link = links[i].a['href']\n",
        "        frase = link.split('/')\n",
        "        file_name = frase[-1]\n",
        "        salvar_arquivo(link, file_name)\n",
        "      elif 'href' and ('.pdf') in texto:\n",
        "        link = links[i].a['href']\n",
        "        frase = link.split('/')\n",
        "        file_name = frase[-1]\n",
        "        salvar_arquivo(link, file_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMCA4R7i/01C4z/DaVtCw6A",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
